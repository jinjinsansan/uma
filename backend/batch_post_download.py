#!/usr/bin/env python3
"""
ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œå³åº§å®Ÿè¡Œãƒãƒƒãƒ
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã§8000é ­ã®ç¾å½¹é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡åé›†
"""
import mysql.connector
import json
import time
import os
from datetime import datetime

def post_download_batch():
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã®æœ€é©åŒ–ãƒãƒƒãƒ"""
    start_time = time.time()
    print("ğŸš€ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œãƒãƒƒãƒå‡¦ç†é–‹å§‹")
    print(f"ğŸ¯ ç›®æ¨™: ç¾å½¹é¦¬8,000é ­ã®éå»5èµ°ãƒ‡ãƒ¼ã‚¿")
    print(f"ğŸ• é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # äº‹å‰ãƒ†ã‚¹ãƒˆ
    if not test_database_performance():
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ€§èƒ½ãƒ†ã‚¹ãƒˆå¤±æ•—")
        return
    
    # æ—¢å­˜é¦¬åèª­ã¿è¾¼ã¿
    existing_horses = load_existing_horses_fast()
    print(f"ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_horses):,}é ­")
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
    output_file = f"data/post_download_batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    # ãƒŠãƒ¬ãƒƒã‚¸æ§‹é€ 
    knowledge = {
        "meta": {
            "created": datetime.now().isoformat(),
            "version": "post_download_1.0",
            "target_horses": 8000,
            "years_covered": "2020-2025"
        },
        "horses": {}
    }
    
    # MySQLæ¥ç¶šï¼ˆæœ€é©åŒ–è¨­å®šï¼‰
    conn = mysql.connector.connect(
        host='172.25.160.1',
        port=3306,
        user='root',
        password='04050405Aoi-',
        database='mykeibadb',
        charset='utf8mb4',
        autocommit=True,
        buffered=True,
        use_unicode=True,
        sql_mode='TRADITIONAL'
    )
    
    try:
        total_processed = 0
        target_horses = 8000
        
        # å¹´åº¦åˆ¥åŠ¹ç‡å‡¦ç†
        years = ['2025', '2024', '2023', '2022', '2021', '2020']
        horses_per_year = target_horses // len(years)
        
        for year in years:
            if total_processed >= target_horses:
                break
            
            print(f"\nğŸ“… {year}å¹´å‡¦ç†é–‹å§‹ (ç›®æ¨™: {horses_per_year}é ­)")
            year_start = time.time()
            
            # å¹´åº¦åˆ¥é¦¬åå–å¾—ï¼ˆæœ€é©åŒ–ã‚¯ã‚¨ãƒªï¼‰
            year_horses = get_year_horses_optimized(conn, year, existing_horses, horses_per_year)
            print(f"   ğŸ {year}å¹´æ–°è¦é¦¬: {len(year_horses)}é ­")
            
            # é¦¬ãƒ‡ãƒ¼ã‚¿ä¸€æ–‰å‡¦ç†
            year_processed = process_horses_batch(conn, year_horses, knowledge)
            total_processed += year_processed
            
            # æ—¢å­˜ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆé‡è¤‡å›é¿ï¼‰
            existing_horses.update(year_horses[:year_processed])
            
            year_time = time.time() - year_start
            print(f"   âœ… {year}å¹´å®Œäº†: {year_processed}é ­, {year_time/60:.1f}åˆ†")
            
            # é€²æ—ä¿å­˜
            save_progress(knowledge, output_file, total_processed)
            
            print(f"ğŸ“Š ç´¯è¨ˆé€²æ—: {total_processed:,}/{target_horses:,}é ­ ({total_processed/target_horses*100:.1f}%)")
        
        # æœ€çµ‚å®Œæˆ
        finalize_batch(knowledge, output_file, total_processed)
        
        total_time = time.time() - start_time
        print("\n" + "="*70)
        print("ğŸ‰ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œãƒãƒƒãƒå‡¦ç†å®Œäº†!")
        print(f"ğŸ“Š æœ€çµ‚çµæœ:")
        print(f"   - æ–°è¦è¿½åŠ : {total_processed:,}é ­")
        print(f"   - ç·ãƒ‡ãƒ¼ã‚¿: {len(existing_horses)+total_processed:,}é ­")
        print(f"   - å®Ÿè¡Œæ™‚é–“: {total_time/3600:.1f}æ™‚é–“")
        print(f"   - å‡¦ç†é€Ÿåº¦: {total_processed/(total_time/3600):.0f}é ­/æ™‚é–“")
        print("="*70)
        
    except Exception as e:
        print(f"âŒ ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        save_progress(knowledge, output_file, total_processed, error=True)
    finally:
        conn.close()

def test_database_performance():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ€§èƒ½äº‹å‰ãƒ†ã‚¹ãƒˆ"""
    try:
        conn = mysql.connector.connect(
            host='172.25.160.1',
            port=3306,
            user='root',
            password='04050405Aoi-',
            database='mykeibadb',
            charset='utf8mb4',
            connect_timeout=10
        )
        
        cursor = conn.cursor()
        start_time = time.time()
        cursor.execute('SELECT COUNT(*) FROM umagoto_race_joho LIMIT 1')
        result = cursor.fetchone()
        query_time = time.time() - start_time
        
        cursor.close()
        conn.close()
        
        print(f"ğŸ“Š æ€§èƒ½ãƒ†ã‚¹ãƒˆ: {query_time:.3f}ç§’, ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {result[0]:,}ä»¶")
        
        if query_time < 2.0:
            print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ€§èƒ½è‰¯å¥½")
            return True
        else:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ€§èƒ½è¦æ”¹å–„")
            return False
            
    except Exception as e:
        print(f"âŒ æ€§èƒ½ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def load_existing_horses_fast():
    """æ—¢å­˜é¦¬åã®é«˜é€Ÿèª­ã¿è¾¼ã¿"""
    existing_file = "data/dlogic_raw_knowledge.json"
    if not os.path.exists(existing_file):
        return set()
    
    try:
        with open(existing_file, 'r', encoding='utf-8') as f:
            # æœ€åˆã®2MBã‹ã‚‰é¦¬åæŠ½å‡º
            chunk = f.read(2 * 1024 * 1024)
            import re
            horse_names = re.findall(r'"([^"]{2,20})": \{', chunk)
            return set(horse_names)
    except:
        return set()

def get_year_horses_optimized(conn, year, existing_horses, limit):
    """å¹´åº¦é¦¬åã®æœ€é©åŒ–å–å¾—"""
    cursor = conn.cursor(dictionary=True)
    
    try:
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ´»ç”¨ã®æœ€é©åŒ–ã‚¯ã‚¨ãƒª
        query = """
            SELECT DISTINCT BAMEI
            FROM umagoto_race_joho 
            WHERE KAISAI_NEN = %s
            AND BAMEI IS NOT NULL 
            AND BAMEI <> ''
            AND LENGTH(BAMEI) > 1
            LIMIT %s
        """
        
        cursor.execute(query, (year, limit * 3))
        results = cursor.fetchall()
        
        # æ—¢å­˜é¦¬é™¤å¤–
        new_horses = []
        for row in results:
            if row['BAMEI'] not in existing_horses and len(new_horses) < limit:
                new_horses.append(row['BAMEI'])
        
        return new_horses
        
    except Exception as e:
        print(f"âŒ {year}å¹´é¦¬åå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []
    finally:
        cursor.close()

def process_horses_batch(conn, horse_names, knowledge):
    """é¦¬ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬å‡¦ç†"""
    processed = 0
    
    for horse_name in horse_names:
        horse_data = extract_horse_races(conn, horse_name)
        if horse_data:
            knowledge["horses"][horse_name] = horse_data
            processed += 1
            
            if processed % 100 == 0:
                print(f"   ğŸ“ˆ {processed}é ­å‡¦ç†å®Œäº†...")
    
    return processed

def extract_horse_races(conn, horse_name):
    """é¦¬ã®éå»ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
    cursor = conn.cursor(dictionary=True)
    
    try:
        query = """
            SELECT 
                KAKUTEI_CHAKUJUN as finish,
                TANSHO_ODDS as odds,
                TANSHO_NINKIJUN as popularity,
                SOHA_TIME as time,
                KAISAI_NEN as year,
                KAISAI_GAPPI as date
            FROM umagoto_race_joho 
            WHERE BAMEI = %s
            AND KAISAI_NEN >= '2020'
            AND KAKUTEI_CHAKUJUN IS NOT NULL
            ORDER BY KAISAI_NEN DESC, KAISAI_GAPPI DESC
            LIMIT 5
        """
        
        cursor.execute(query, (horse_name,))
        races = cursor.fetchall()
        
        if races:
            return {
                "name": horse_name,
                "race_count": len(races),
                "past_races": races,
                "processed_at": datetime.now().isoformat()
            }
            
    except Exception as e:
        print(f"âŒ {horse_name}ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        cursor.close()
    
    return None

def save_progress(knowledge, output_file, count, error=False):
    """é€²æ—ä¿å­˜"""
    knowledge["meta"]["current_count"] = count
    knowledge["meta"]["last_save"] = datetime.now().isoformat()
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(knowledge, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ é€²æ—ä¿å­˜: {count}é ­")

def finalize_batch(knowledge, output_file, count):
    """æœ€çµ‚å®Œæˆ"""
    knowledge["meta"]["final_count"] = count
    knowledge["meta"]["completed"] = datetime.now().isoformat()
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(knowledge, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    post_download_batch()