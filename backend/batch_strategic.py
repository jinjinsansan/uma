#!/usr/bin/env python3
"""
æˆ¦ç•¥çš„ãƒãƒƒãƒå‡¦ç† - GROUP BYã‚’å›é¿ã—ãŸåŠ¹ç‡çš„ãªå¤§é‡å‡¦ç†
281ä¸‡ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ®µéšçš„ã«é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
"""
import mysql.connector
import json
import time
import os
from datetime import datetime
from collections import defaultdict

def strategic_batch_process():
    """æˆ¦ç•¥çš„ãƒãƒƒãƒå‡¦ç†ãƒ¡ã‚¤ãƒ³"""
    start_time = time.time()
    print("ğŸš€ æˆ¦ç•¥çš„ãƒãƒƒãƒå‡¦ç†é–‹å§‹")
    print(f"ğŸ• é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
    output_file = f"data/strategic_batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    # æ—¢å­˜é¦¬åã®åŠ¹ç‡çš„èª­ã¿è¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
    existing_horses = load_existing_horses_efficiently()
    print(f"ğŸ“Š æ—¢å­˜é¦¬åæ•°: {len(existing_horses):,}é ­")
    
    # æ–°ã—ã„ãƒŠãƒ¬ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿
    new_knowledge = {
        "meta": {
            "created": datetime.now().isoformat(),
            "version": "strategic_1.0",
            "source": "strategic_batch",
            "target_records": 0
        },
        "horses": {}
    }
    
    # ç›´æ¥MySQLæ¥ç¶š
    conn = mysql.connector.connect(
        host='172.25.160.1',
        port=3306,
        user='root',
        password='04050405Aoi-',
        database='mykeibadb',
        charset='utf8mb4',
        autocommit=True,
        buffered=True  # é‡è¦ï¼šçµæœã‚’ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
    )
    
    try:
        processed_horses = 0
        total_records = 0
        
        # æˆ¦ç•¥1: å¹´ã”ã¨ã«å‡¦ç†ã—ã¦è² è·åˆ†æ•£
        years = ['2024', '2023', '2022', '2021', '2020']
        
        for year in years:
            print(f"\nğŸ—“ï¸ {year}å¹´ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")
            year_start = time.time()
            
            # ãã®å¹´ã®é¦¬åã‚’åŠ¹ç‡çš„ã«å–å¾—ï¼ˆGROUP BYãªã—ï¼‰
            horses_this_year = get_horses_by_year_efficient(conn, year, existing_horses)
            print(f"   {year}å¹´: {len(horses_this_year)}é ­ã®æ–°è¦é¦¬ã‚’ç™ºè¦‹")
            
            # å„é¦¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’å€‹åˆ¥å–å¾—
            year_processed = 0
            for horse_name in horses_this_year:
                if year_processed >= 200:  # å¹´ã‚ãŸã‚Šæœ€å¤§200é ­ã¾ã§
                    break
                    
                horse_data = extract_horse_data_efficient(conn, horse_name, year)
                if horse_data:
                    new_knowledge["horses"][horse_name] = horse_data
                    year_processed += 1
                    total_records += len(horse_data.get('races', []))
                    
                    if year_processed % 20 == 0:
                        print(f"   ğŸ“ˆ {year}å¹´: {year_processed}é ­å‡¦ç†å®Œäº†")
            
            processed_horses += year_processed
            year_time = time.time() - year_start
            print(f"   âœ… {year}å¹´å®Œäº†: {year_processed}é ­, {year_time:.1f}ç§’")
            
            # ä¸­é–“ä¿å­˜
            if processed_horses % 100 == 0:
                save_intermediate_data(new_knowledge, output_file, processed_horses)
            
            # æ—©æœŸçµ‚äº†æ¡ä»¶ï¼ˆååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒé›†ã¾ã£ãŸå ´åˆï¼‰
            if processed_horses >= 1000:
                print(f"ğŸ¯ ç›®æ¨™é”æˆ: {processed_horses}é ­å‡¦ç†å®Œäº†")
                break
        
        # æœ€çµ‚ä¿å­˜
        new_knowledge["meta"]["horses_count"] = processed_horses
        new_knowledge["meta"]["total_records"] = total_records
        new_knowledge["meta"]["completed"] = datetime.now().isoformat()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(new_knowledge, f, ensure_ascii=False, indent=2)
        
        elapsed = time.time() - start_time
        
        print("\n" + "="*60)
        print("ğŸ‰ æˆ¦ç•¥çš„ãƒãƒƒãƒå‡¦ç†å®Œäº†ï¼")
        print(f"ğŸ“Š å‡¦ç†çµæœ:")
        print(f"   - æ–°è¦è¿½åŠ é¦¬: {processed_horses:,}é ­")
        print(f"   - ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_records:,}ä»¶")
        print(f"   - å®Ÿè¡Œæ™‚é–“: {elapsed/60:.1f}åˆ†")
        print(f"   - å‡¦ç†é€Ÿåº¦: {processed_horses/(elapsed/60):.1f}é ­/åˆ†")
        print(f"   - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        conn.close()

def load_existing_horses_efficiently():
    """æ—¢å­˜é¦¬åã®ã¿ã‚’åŠ¹ç‡çš„ã«èª­ã¿è¾¼ã¿"""
    existing_horses = set()
    existing_file = "data/dlogic_raw_knowledge.json"
    
    if not os.path.exists(existing_file):
        return existing_horses
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        file_size = os.path.getsize(existing_file)
        if file_size > 50 * 1024 * 1024:  # 50MBä»¥ä¸Šãªã‚‰éƒ¨åˆ†èª­ã¿è¾¼ã¿
            print("ğŸ“‹ å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’éƒ¨åˆ†èª­ã¿è¾¼ã¿ä¸­...")
            with open(existing_file, 'r', encoding='utf-8') as f:
                # æœ€åˆã®1MBã‹ã‚‰é¦¬åã‚’æŠ½å‡º
                chunk = f.read(1024 * 1024)
                import re
                horse_names = re.findall(r'"([^"]+)": \{', chunk)
                existing_horses = set(horse_names)
        else:
            print("ğŸ“‹ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
            with open(existing_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing_horses = set(data.get("horses", {}).keys())
    except Exception as e:
        print(f"âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    return existing_horses

def get_horses_by_year_efficient(conn, year, existing_horses):
    """å¹´ã”ã¨ã«é¦¬åã‚’åŠ¹ç‡çš„ã«å–å¾—ï¼ˆGROUP BYãªã—ï¼‰"""
    cursor = conn.cursor(dictionary=True)
    
    # DISTINCTåˆ©ç”¨ã§GROUP BYã‚’å›é¿
    query = """
        SELECT DISTINCT BAMEI
        FROM umagoto_race_joho 
        WHERE KAISAI_NEN = %s
        AND BAMEI IS NOT NULL 
        AND BAMEI <> ''
        AND KAKUTEI_CHAKUJUN IS NOT NULL
        LIMIT 1000
    """
    
    cursor.execute(query, (year,))
    results = cursor.fetchall()
    cursor.close()
    
    # æ—¢å­˜é¦¬ã‚’é™¤å¤–
    new_horses = [row['BAMEI'] for row in results if row['BAMEI'] not in existing_horses]
    return new_horses[:300]  # å¹´ã‚ãŸã‚Šæœ€å¤§300é ­

def extract_horse_data_efficient(conn, horse_name, primary_year):
    """åŠ¹ç‡çš„ãªé¦¬ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
    cursor = conn.cursor(dictionary=True)
    
    try:
        # ç‰¹å®šã®é¦¬ã®æœ€æ–°ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—
        query = """
            SELECT 
                KAKUTEI_CHAKUJUN as finish,
                TANSHO_ODDS as odds,
                TANSHO_NINKIJUN as popularity,
                KAISAI_NEN as year,
                KAISAI_GAPPI as date,
                SOHA_TIME as time
            FROM umagoto_race_joho 
            WHERE BAMEI = %s
            AND KAISAI_NEN >= '2020'
            AND KAKUTEI_CHAKUJUN IS NOT NULL
            ORDER BY KAISAI_NEN DESC, KAISAI_GAPPI DESC
            LIMIT 12
        """
        
        cursor.execute(query, (horse_name,))
        races = cursor.fetchall()
        
        if not races:
            return None
        
        # åŸºæœ¬çµ±è¨ˆè¨ˆç®—
        total_races = len(races)
        wins = sum(1 for race in races if race.get('finish') == 1)
        avg_odds = sum(float(race.get('odds', 0)) for race in races if race.get('odds')) / max(1, total_races)
        
        return {
            "name": horse_name,
            "primary_year": primary_year,
            "total_races": total_races,
            "wins": wins,
            "win_rate": round(wins / total_races * 100, 1) if total_races > 0 else 0,
            "avg_odds": round(avg_odds, 1),
            "races": races[:8],  # æœ€æ–°8ãƒ¬ãƒ¼ã‚¹ã®ã¿ä¿å­˜
            "processed_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ {horse_name}ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None
    finally:
        cursor.close()

def save_intermediate_data(knowledge, output_file, count):
    """ä¸­é–“ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
    knowledge["meta"]["intermediate_save"] = datetime.now().isoformat()
    knowledge["meta"]["current_count"] = count
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(knowledge, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ä¸­é–“ä¿å­˜å®Œäº†: {count}é ­")

if __name__ == "__main__":
    strategic_batch_process()